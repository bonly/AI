{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bonly/AI/blob/main/torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#安装"
      ],
      "metadata": {
        "id": "JnwFNxPaGE3s"
      },
      "id": "JnwFNxPaGE3s"
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果没有nvidia显示卡的：\n",
        "pip3 install torch torchvision #torchaudio"
      ],
      "metadata": {
        "id": "O0taX72wfrc3"
      },
      "id": "O0taX72wfrc3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d94e4dd-df73-4b75-a468-ca41f92b4749",
      "metadata": {
        "id": "4d94e4dd-df73-4b75-a468-ca41f92b4749"
      },
      "outputs": [],
      "source": [
        "!pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu129"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#核心概念"
      ],
      "metadata": {
        "id": "S7o8bA7jF0xq"
      },
      "id": "S7o8bA7jF0xq"
    },
    {
      "cell_type": "markdown",
      "id": "d633c668-102d-4eef-adb2-36ad5a354075",
      "metadata": {
        "id": "d633c668-102d-4eef-adb2-36ad5a354075"
      },
      "source": [
        "## 张量（Tensor） ：类似于 NumPy 的多维数组，不同的是 PyTorch 的张量可以利用 GPU 加速计算。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43775f8f-a11e-4ee3-ba1a-593f0b88840c",
      "metadata": {
        "id": "43775f8f-a11e-4ee3-ba1a-593f0b88840c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "x = torch.tensor([1, 2, 3])  # 创建一个一维张量\n",
        "y = torch.tensor([[4, 5], [6, 7]])  # 创建一个二维张量"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2291aa7d-207f-4a5c-9164-89e381af954b",
      "metadata": {
        "id": "2291aa7d-207f-4a5c-9164-89e381af954b"
      },
      "source": [
        "## 自动求导(Autograd):\n",
        ">PyTorch的自动微分引擎，能够记录张量操作，并自动计算梯度。\n",
        "### .requires_grad_(True):\n",
        ">设置此属性为True，表示需要跟踪该张量的操作，以便后续自动求导。\n",
        "### .backward():\n",
        ">在计算图的某个张量上调用 **.backward()，PyTorch会自动沿着计算历史，计算所有依赖张量的梯度，并将结果存储在每个张量的.grad** 属性中。\n",
        "### 梯度(Gradient):\n",
        ">函数的导数，表示函数值变化对输入变量变化的敏感程度。在深度学习中，梯度用于指导模型参数的更新方向，使模型朝着最小化损失函数的方向优化。\n",
        "\n",
        "\n",
        "> 自动求导（Autograd） ：PyTorch 的自动求导功能可以自动计算张量的梯度，这对于神经网络的训练至关重要。在张量上调用 .backward() 方法可以计算梯度；使用 torch.no_grad() 可以停止梯度追踪。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a2af515-c9fa-47d5-8d42-7140a10c6d8a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a2af515-c9fa-47d5-8d42-7140a10c6d8a",
        "outputId": "fdec846e-3a81-46fd-e795-a61c48720d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3., 3.],\n",
            "        [3., 3.]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.ones(2, 2, requires_grad=True) #创建一个2x2的全1张量，并梯度跟踪\n",
        "y = x * 3\n",
        "y.backward(torch.ones_like(x)) #创建一个与输入张量（tensor）形状相同、但所有元素值为1的新张量\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "143f23cf-c7fe-4bfb-af4f-6e25cd48302d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "143f23cf-c7fe-4bfb-af4f-6e25cd48302d",
        "outputId": "56fa6a44-821a-496f-91b8-998b5803aae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(864.)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 创建一个需要求导的张量\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "# 进行一系列计算\n",
        "y = x + 2\n",
        "z = y * y * 3\n",
        "# 目标函数\n",
        "target = torch.tensor(30.0)\n",
        "# 计算损失\n",
        "loss = (z - target).pow(2)\n",
        "\n",
        "# 自动求导\n",
        "loss.backward()\n",
        "\n",
        "# 打印梯度\n",
        "print(x.grad) # 输出: tensor(48.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3efd3770-94ad-4a10-942c-a46178d64af7",
      "metadata": {
        "id": "3efd3770-94ad-4a10-942c-a46178d64af7"
      },
      "source": [
        ">x是输入，y, z, loss是中间变量和最终的损失。通过调用loss.backward()，PyTorch自动计算了x的梯度，并存储在x.grad中"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#构建神经网络"
      ],
      "metadata": {
        "id": "CNaClUwNGaGv"
      },
      "id": "CNaClUwNGaGv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##定义网络结构 ：使用 torch.nn.Module 来定义神经网络的结构"
      ],
      "metadata": {
        "id": "FerBan-9Ghye"
      },
      "id": "FerBan-9Ghye"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 10)  # 输入层到隐藏层的连接，输入维度为4，输出维度为10\n",
        "        self.fc2 = nn.Linear(10, 4)  # 隐藏层到输出层的连接，输入维度为10，输出维度为4\n",
        "        self.relu = nn.ReLU()  # 激活函数 ReLU：如果输入大于 0，输出等于输入；否则输出 0。\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "net = Net() #创建一个 Net 类的实例（即一个具体的神经网络对象）"
      ],
      "metadata": {
        "id": "QobNOE8mGqo3"
      },
      "id": "QobNOE8mGqo3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### self.fc1 = nn.Linear(4, 10)\n",
        "\n",
        "* 作用：定义第一个全连接层（也叫线性层）。\n",
        "* 简单解释：\n",
        "\n",
        "    * 输入维度是 4，输出维度是 10。\n",
        "    * 例子：如果输入是 [a, b, c, d]（4个数），这层会输出 10 个数。"
      ],
      "metadata": {
        "id": "vYYHN-QAXoef"
      },
      "id": "vYYHN-QAXoef"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### self.fc2 = nn.Linear(10, 4)\n",
        "\n",
        "* 作用：定义第二个全连接层。\n",
        "* 简单解释：\n",
        "\n",
        "    * 输入维度是 10（接收上一层的输出），输出维度是 4。\n",
        "    * 例子：将 10 个数转换为 4 个数。"
      ],
      "metadata": {
        "id": "elt6wdSwXr48"
      },
      "id": "elt6wdSwXr48"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### self.relu = nn.ReLU()\n",
        "\n",
        "* 作用：定义 ReLU 激活函数。\n",
        "* 简单解释：\n",
        "\n",
        "    * ReLU：如果输入大于 0，输出等于输入；否则输出 0。\n",
        "    * 例子：ReLU(2) = 2，ReLU(-1) = 0。"
      ],
      "metadata": {
        "id": "i7sNMIpeXd0A"
      },
      "id": "i7sNMIpeXd0A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### forward 函数\n",
        "* 作用：定义数据如何通过网络（即“前向传播”）。\n",
        "* 简单解释：\n",
        "\n",
        "    * 输入 x 先经过 fc1 层，再经过 ReLU 激活，最后经过 fc2 层。"
      ],
      "metadata": {
        "id": "Aog-bkQZXRWe"
      },
      "id": "Aog-bkQZXRWe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 整体流程\n",
        "\n",
        "* 输入 4 个数 → fc1 层 → 得到 10 个数\n",
        "\n",
        "* 10 个数 → ReLU 激活 → 非负的 10 个数\n",
        "\n",
        "* 10 个数 → fc2 层 → 得到 4 个数（最终输出）"
      ],
      "metadata": {
        "id": "rmO6YsS3X8Pu"
      },
      "id": "rmO6YsS3X8Pu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 定义损失函数和优化器 ：\n",
        "选择合适的损失函数和优化器对于模型的训练效果至关重要。\n",
        "\n",
        "常见的损失函数有均方误差损失（MSELoss）、交叉熵损失（CrossEntropyLoss）等；\n",
        "\n",
        "常用的优化器有随机梯度下降（SGD）、Adam 等。"
      ],
      "metadata": {
        "id": "5lvotxjcYdzY"
      },
      "id": "5lvotxjcYdzY"
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()  # 定义均方误差损失函数\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)  # 定义随机梯度下降优化器，学习率为0.01"
      ],
      "metadata": {
        "id": "EFPLWrQYY6cc"
      },
      "id": "EFPLWrQYY6cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 损失函数（Loss Function）\n",
        "代码：\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "含义：\n",
        "\n",
        "* 损失函数用来衡量模型的预测值和真实值之间的差距。\n",
        "\n",
        "* MSELoss 是“均方误差损失”的简称，计算的是预测值和真实值之间的平方差的平均值。\n",
        "\n",
        "* 举例：\n",
        "假设你预测一个房子的价格是 300 万元，实际价格是 280 万元，MSELoss 会计算 (300 - 280)^2 = 400，然后对所有样本的平方差取平均值，作为模型的“错误程度”。\n",
        "\n",
        "为什么用均方误差？\n",
        "\n",
        "* 平方操作会放大较大的错误，让模型更关注大的误差。"
      ],
      "metadata": {
        "id": "pia4d36ZZycz"
      },
      "id": "pia4d36ZZycz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 优化器（Optimizer）\n",
        "\n",
        "代码：\n",
        "\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "含义：\n",
        "\n",
        "* 优化器用来根据损失函数的结果，调整模型的参数（权重和偏置），使模型更准确。\n",
        "\n",
        "* SGD 是“随机梯度下降”的简称，是最常用的优化方法之一。\n",
        "\n",
        "   * net.parameters()：模型中需要调整的所有参数。\n",
        "   \n",
        "   * lr=0.01：学习率（learning rate），决定每次调整参数的步子大小。\n",
        "\n",
        "      * 学习率太大：可能调整过头，模型不稳定。\n",
        "\n",
        "      * 学习率太小：调整太慢，训练时间长。\n",
        "\n",
        "举例：\n",
        "\n",
        "- 假设模型预测错了，损失函数告诉你“错了 20 万元”，SGD 会根据学习率（0.01）调整参数，比如把权重减少 0.01 * 20 = 0.2 单位，让下次预测更接近真实值。"
      ],
      "metadata": {
        "id": "82TVIqk2Z9PD"
      },
      "id": "82TVIqk2Z9PD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 总结\n",
        "\n",
        "* 损失函数告诉模型“错了多少”。\n",
        "\n",
        "* 优化器告诉模型“如何改进”。\n",
        "\n",
        "这两者配合使用，让模型在训练过程中不断提高准确度。\n",
        "\n",
        "如果你想更直观地理解，可以想象成：\n",
        "\n",
        "* 损失函数是“考试成绩单”，告诉你哪里错了。\n",
        "\n",
        "* 优化器是“学习方法”，告诉你如何改进成绩。"
      ],
      "metadata": {
        "id": "syM_HzLqaKMh"
      },
      "id": "syM_HzLqaKMh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 训练网络 ：使用训练数据对网络进行训练，通过前向传播计算输出，计算损失函数的值，然后进行反向传播更新网络的参数。"
      ],
      "metadata": {
        "id": "OL2x4k6NZNMk"
      },
      "id": "OL2x4k6NZNMk"
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1000):  # 训练1000个周期\n",
        "    optimizer.zero_grad()  # 清空梯度\n",
        "    output = net(x_data)  # 前向传播\n",
        "    loss = criterion(output, y_data)  # 计算损失\n",
        "    loss.backward()  # 反向传播\n",
        "    optimizer.step()  # 更新参数\n",
        "    if epoch % 100 == 99:  # 每100个周期打印一次损失\n",
        "        print('Epoch: {}, Loss: {}'.format(epoch+1, loss.item()))"
      ],
      "metadata": {
        "id": "PH3tgVEwZXw4"
      },
      "id": "PH3tgVEwZXw4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### optimizer.zero_grad()\n",
        "\n",
        "* 含义：每次训练前，清空之前的梯度（误差反馈）\n",
        "\n",
        "### 损失函数（loss = criterion(output, y_data)）\n",
        "\n",
        "* 含义：计算模型预测结果（output）和真实答案（y_data）之间的差距。\n",
        "\n",
        "### 反向传播（loss.backward()）\n",
        "\n",
        "* 含义：根据损失，计算每个参数（权重）需要调整的方向和大小。\n",
        "\n",
        "* 例子：告诉模型“猜错是因为哪些地方没学好”，并标记出来。\n",
        "\n",
        "### 更新参数（optimizer.step()）\n",
        "\n",
        "* 含义：根据反向传播的结果，实际调整模型的参数（权重）。\n",
        "\n",
        "* 例子：根据标记，调整模型的“学习笔记”，让下次猜得更准。"
      ],
      "metadata": {
        "id": "1S3-9q0oc6V-"
      },
      "id": "1S3-9q0oc6V-"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}