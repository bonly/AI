{
 "cells": [
     {
      "cell_type": "markdown",
      "id": "c4e1b477-982d-4459-8a6e-a491b8d87e65",
      "metadata": {
        "id": "c4e1b477-982d-4459-8a6e-a491b8d87e65"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bonly/AI/blob/main/torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d94e4dd-df73-4b75-a468-ca41f92b4749",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu129"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d633c668-102d-4eef-adb2-36ad5a354075",
   "metadata": {},
   "source": [
    "# 张量（Tensor） ：类似于 NumPy 的多维数组，不同的是 PyTorch 的张量可以利用 GPU 加速计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43775f8f-a11e-4ee3-ba1a-593f0b88840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor([1, 2, 3])  # 创建一个一维张量\n",
    "y = torch.tensor([[4, 5], [6, 7]])  # 创建一个二维张量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2291aa7d-207f-4a5c-9164-89e381af954b",
   "metadata": {},
   "source": [
    "# 自动求导(Autograd):\n",
    "PyTorch的自动微分引擎，能够记录张量操作，并自动计算梯度。\n",
    "# .requires_grad_(True):\n",
    "设置此属性为True，表示需要跟踪该张量的操作，以便后续自动求导。\n",
    "# .backward():\n",
    "在计算图的某个张量上调用 **.backward()，PyTorch会自动沿着计算历史，计算所有依赖张量的梯度，并将结果存储在每个张量的.grad** 属性中。\n",
    "# 梯度(Gradient):\n",
    "函数的导数，表示函数值变化对输入变量变化的敏感程度。在深度学习中，梯度用于指导模型参数的更新方向，使模型朝着最小化损失函数的方向优化。\n",
    "\n",
    "\n",
    "> 自动求导（Autograd） ：PyTorch 的自动求导功能可以自动计算张量的梯度，这对于神经网络的训练至关重要。在张量上调用 .backward() 方法可以计算梯度；使用 torch.no_grad() 可以停止梯度追踪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a2af515-c9fa-47d5-8d42-7140a10c6d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True) #创建一个2x2的全1张量，并梯度跟踪\n",
    "y = x * 3\n",
    "y.backward(torch.ones_like(x))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "143f23cf-c7fe-4bfb-af4f-6e25cd48302d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(864.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个需要求导的张量\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "# 进行一系列计算\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "# 目标函数\n",
    "target = torch.tensor(30.0)\n",
    "# 计算损失\n",
    "loss = (z - target).pow(2)\n",
    "\n",
    "# 自动求导\n",
    "loss.backward()\n",
    "\n",
    "# 打印梯度\n",
    "print(x.grad) # 输出: tensor(48.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efd3770-94ad-4a10-942c-a46178d64af7",
   "metadata": {},
   "source": [
    ">>x是输入，y, z, loss是中间变量和最终的损失。通过调用loss.backward()，PyTorch自动计算了x的梯度，并存储在x.grad中"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
